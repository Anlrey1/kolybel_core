# n8n_docs.py ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞, –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ –ø–æ–∏—Å–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ n8n
"""–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π n8n-io/n8n-docs, –ø–∞—Ä—Å–∏—Ç Markdown –∏ –∫–ª–∞–¥—ë—Ç
—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (—á–∞–Ω–∫–∏) –≤ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—é `n8n_docs`.  
–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –µ—Å–ª–∏ —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –¥–æ–∫—É–º–µ–Ω—Ç
  –Ω–µ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è.
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ —Ä—É—Å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –±–µ–∑ —Ä—É—Å—Å–∫–æ–π –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏
  (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è generate_text_raw) + LRU-–∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤.
- CLI-—Ä–µ–∂–∏–º: `python -m n8n_docs update` –∏ `python -m n8n_docs search`.  
"""
from __future__ import annotations

import hashlib
import io
import json
import logging
import os
import re
import zipfile
from functools import lru_cache
from typing import Dict, Iterable, List, Optional

import requests

from memory_core import MemoryCore
from llm import generate_text_raw

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# === –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ===
N8N_DOCS_ZIP_URL = "https://github.com/n8n-io/n8n-docs/archive/refs/heads/main.zip"
LOCAL_DOCS_DIR = os.path.join("docs", "n8n")
COLLECTION_NAME = "n8n_docs"

# === Regex –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ Markdown ===
MD_FRONTMATTER_RE = re.compile(r"^---[\s\S]*?---\n", re.MULTILINE)
MD_CODEBLOCK_RE = re.compile(r"```[\s\S]*?```", re.MULTILINE)
MD_LINK_RE = re.compile(r"\[(.*?)\]\((.*?)\)")
MD_IMAGE_RE = re.compile(r"!\[[^\]]*\]\([^\)]*\)")

# == –£—Ç–∏–ª–∏—Ç—ã —Ñ–∞–π–ª–æ–≤ ==

def ensure_dirs() -> None:
    os.makedirs(LOCAL_DOCS_DIR, exist_ok=True)


def download_docs_zip() -> bytes:
    logger.info("‚¨áÔ∏è  –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ n8n (zip)‚Ä¶")
    resp = requests.get(N8N_DOCS_ZIP_URL, timeout=60)
    resp.raise_for_status()
    return resp.content


def extract_zip_to_dir(zip_bytes: bytes, target_dir: str) -> None:
    logger.info("üì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞‚Ä¶")
    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:
        # –û—á–∏—Å—Ç–∏–º –ø–∞–ø–∫—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–æ–π
        for root, dirs, files in os.walk(target_dir, topdown=False):
            for file in files:
                try:
                    os.remove(os.path.join(root, file))
                except Exception:
                    pass
            for d in dirs:
                try:
                    os.rmdir(os.path.join(root, d))
                except Exception:
                    pass
        zf.extractall(target_dir)


def iter_markdown_files(root_dir: str) -> Iterable[str]:
    for root, _, files in os.walk(root_dir):
        for f in files:
            if f.lower().endswith(".md"):
                yield os.path.join(root, f)


def read_file_text(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except UnicodeDecodeError:
        with open(path, "r", encoding="latin-1") as f:
            return f.read()


# === –û—á–∏—Å—Ç–∫–∞ –∏ —á–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏–µ ===

def clean_markdown(md: str) -> str:
    md = MD_FRONTMATTER_RE.sub("", md)
    md = MD_CODEBLOCK_RE.sub("", md)
    md = MD_LINK_RE.sub(lambda m: m.group(1), md)
    md = MD_IMAGE_RE.sub("", md)
    lines = [ln.strip() for ln in md.splitlines() if ln.strip()]
    return "\n".join(lines)


def chunk_text(text: str, max_chars: int = 1200, overlap: int = 150) -> List[str]:
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        chunk = text[start:end]
        # —Ä–∞—Å—à–∏—Ä–∏–º –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–∏–º–∏—Ç
        if end < n:
            nl = text.rfind("\n", start, end)
            if nl > start + 200:
                chunk = text[start:nl]
                end = nl
        chunks.append(chunk)
        start = max(end - overlap, end)
    return chunks


# === –ü–µ—Ä–µ–≤–æ–¥ RU‚ÜíEN —Å LRU-–∫—ç—à–µ–º ===

@lru_cache(maxsize=256)
def _translate_ru_to_en_cached(text: str) -> str:
    prompt = (
        "Translate the following user phrase from Russian to English as literally as possible. "
        "Return ONLY the translated text, without quotes or comments.\n\n" + text
    )
    translated = generate_text_raw(prompt).strip() or text
    return translated


def ru_to_en(text: str) -> str:
    if not text:
        return text
    return _translate_ru_to_en_cached(text)


# === –ö–ª–∞—Å—Å –¥–æ—Å—Ç—É–ø–∞ –∫ –∑–Ω–∞–Ω–∏—è–º ===

class N8nKnowledge:
    """–†–∞–±–æ—Ç–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ n8n."""

    def __init__(self, memory: Optional[MemoryCore] = None):
        self.memory = memory or MemoryCore()

    # == –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è ==
    def update_index(self, limit_files: Optional[int] = None) -> Dict[str, int]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é n8n.
        –î–µ–ª–∞–µ—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ Markdown —Ñ–∞–π–ª–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è
        SHA-1 —Ö—ç—à –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ —Ç–∞–∫–æ–π —Ö—ç—à —É–∂–µ –µ—Å—Ç—å –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ‚Äî —Ñ–∞–π–ª
        –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è.
        """
        ensure_dirs()
        zip_bytes = download_docs_zip()
        extract_zip_to_dir(zip_bytes, LOCAL_DOCS_DIR)

        # –∫–æ—Ä–µ–Ω—å, –≤ –∫–æ—Ç–æ—Ä–æ–º —Ä–∞—Å–ø–∞–∫–æ–≤–∞–ª—Å—è –∞—Ä—Ö–∏–≤ (n8n-docs-main)
        root_dir = next(
            (
                os.path.join(LOCAL_DOCS_DIR, d)
                for d in os.listdir(LOCAL_DOCS_DIR)
                if os.path.isdir(os.path.join(LOCAL_DOCS_DIR, d)) and d.startswith("n8n-docs-")
            ),
            LOCAL_DOCS_DIR,
        )

        md_files = list(iter_markdown_files(root_dir))
        if limit_files:
            md_files = md_files[: limit_files]
        logger.info("üìÑ –ù–∞–π–¥–µ–Ω–æ Markdown —Ñ–∞–π–ª–æ–≤: %s", len(md_files))

        skipped_files = 0
        added_chunks = 0
        for filepath in md_files:
            rel_path = os.path.relpath(filepath, root_dir)
            raw_md = read_file_text(filepath)
            cleaned = clean_markdown(raw_md)
            if not cleaned.strip():
                skipped_files += 1
                continue

            file_hash = hashlib.sha1(cleaned.encode("utf-8")).hexdigest()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ö—ç—à–µ–º
            existing = self.memory.query_collection(
                COLLECTION_NAME,
                query="placeholder",  # —Å–∞–º–∞ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ where
                n_results=1,
                where={"file_hash": {"$eq": file_hash}},
            )
            if existing:
                skipped_files += 1
                continue

            url_hint = f"https://docs.n8n.io/{rel_path.replace(os.sep, '/').replace('README.md','')}"
            for idx, chunk in enumerate(chunk_text(cleaned)):
                meta = {
                    "type": "n8n_doc",
                    "source_file": rel_path,
                    "chunk_index": idx,
                    "url": url_hint,
                    "file_hash": file_hash,
                }
                self.memory.store_in_collection(COLLECTION_NAME, chunk, metadata=meta)
                added_chunks += 1

        logger.info(
            "‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ß–∞–Ω–∫–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: %s; —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ: %s",
            added_chunks,
            skipped_files,
        )
        return {
            "files_processed": len(md_files),
            "files_skipped": skipped_files,
            "chunks_added": added_chunks,
        }

    # == –ü–æ–∏—Å–∫ ==
    def search(self, query: str, n_results: int = 8) -> List[Dict]:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞ (–≥—Ä—É–±–æ)
        ru_chars = sum(1 for ch in query if "–∞" <= ch <= "—è" or "–ê" <= ch <= "–Ø" or ch in "—ë–Å")
        lat_chars = sum(1 for ch in query if "a" <= ch <= "z" or "A" <= ch <= "Z")
        if ru_chars > lat_chars:
            query_en = ru_to_en(query)
        else:
            query_en = query

        results = self.memory.query_collection(
            COLLECTION_NAME,
            query_en,
            n_results=n_results,
            where={"type": {"$eq": "n8n_doc"}},
        )

        for r in results:
            snippet = r.get("content", "")
            r["snippet"] = snippet[:500] + "‚Ä¶" if len(snippet) > 500 else snippet
        return results


# === CLI ===

def _cli() -> None:
    import argparse

    parser = argparse.ArgumentParser(description="n8n docs indexer/search")
    sub = parser.add_subparsers(dest="cmd")

    p_upd = sub.add_parser("update", help="Download and (re)index n8n docs")
    p_upd.add_argument("--limit", type=int, default=None, help="Limit number of md files to index (debug)")

    p_search = sub.add_parser("search", help="Search in indexed n8n docs")
    p_search.add_argument("query", type=str, help="Query string (RU or EN)")
    p_search.add_argument("--k", type=int, default=8, help="Number of results to return")

    args = parser.parse_args()
    kn = N8nKnowledge()

    if args.cmd == "update":
        stats = kn.update_index(limit_files=args.limit)
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    elif args.cmd == "search":
        hits = kn.search(args.query, n_results=args.k)
        print(json.dumps(hits, ensure_ascii=False, indent=2))
    else:
        parser.print_help()


if __name__ == "__main__":
    _cli()
